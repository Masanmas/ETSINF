{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analisis_2Datos.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPdC0LyXDh2TmiXsRt8DY/l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masanmas/ETSINF/blob/master/Analisis_2Datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCDSYK3W6uTk"
      },
      "source": [
        "import pandas as pd #Tratamiento dataset formato CSV.\n",
        "import numpy as np #Uso de matrices.\n",
        "import torch #Librería Redes Neuronales.\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataSet, DataLoader\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split #Separar el conjunto de datos en Test y Entrenamiento.\n",
        "\n",
        "from collab import drive #Cargar archivos desde drive."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZONG1byB9Wfz"
      },
      "source": [
        "DRIVE_PATH = '/content/drive/my drive/DataSet_2Types.csv'\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "MAX_LEN = 200\n",
        "BATCH_SIZE = 16 #Para no lanzar los datos de golpe, los lanzaremos en paquetes de 16.\n",
        "\n",
        "NCLASSES = 2 #Topics: ['TRAVEL'=0, 'STYLE & BEAUTY'=1]\n",
        "\n",
        "PRETRAINED_BERT_MODEL = 'bert-base-cased'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKm7kmmd_e_y"
      },
      "source": [
        "#INIT\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = (\"GPU\" if torch.cuda.is_available() else \"CPU\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tIDztYU8-DD"
      },
      "source": [
        "#LOAD DATASET\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv(DRIVE_PATH)\n",
        "df = df[0:10000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg72IbjXGmkS"
      },
      "source": [
        "#INIT BERT TOKENIZER\n",
        "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_BERT_MODEL)\n",
        "\n",
        "sample_txt = \"Places you had to visit when travelling to New York\"\n",
        "tokens = tokenizer.tokenize(sample_txt)\n",
        "token_ids = tokens.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print('Frase: ', sample_txt)\n",
        "print('Tokens: ', tokens)\n",
        "print('Token IDs: ', token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTo80o-cJAQX"
      },
      "source": [
        "#CODIFICACIÓN BERT\n",
        "encoder = tokenizer.encoder_plus(\n",
        "    sample_txt,\n",
        "    max_length = 25,\n",
        "    truncation = True,\n",
        "    add_special_tokens = True,\n",
        "    return_token_type_ids = False,\n",
        "    padding = 25,\n",
        "    return_attention_mask = True,\n",
        "    return_tensors = 'pt' \n",
        ")\n",
        "\n",
        "encoder.keys() #['input_ids', 'attention_mask']\n",
        "\n",
        "print(tokenizer.convert_ids_to_tokens(encoder['input_ids'][0])) #Frase Tokenizada\n",
        "print(encoding['input_ids'][0]) #tensor([list(ids)])\n",
        "print(encoding['attention_mask'][0]) #tensor([list([1|0])]) - Lista de elementos a los que prestar attención"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hWZ9qwT8nbp"
      },
      "source": [
        "#Create DATASET\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, labels, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "      review = str(self.reviews[item])\n",
        "      label = self.labels[item]\n",
        "\n",
        "      encoding = tokenizer.encoder_plus(\n",
        "        review,\n",
        "        max_length = self.max_len,\n",
        "        truncation = True,\n",
        "        add_special_tokens = True,\n",
        "        return_token_type_ids = False,\n",
        "        padding = 25,\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = 'pt' \n",
        "      )\n",
        "\n",
        "      return {\n",
        "          'review': review,\n",
        "          'input_ids': encoding['inputs_ids'].flatten(),\n",
        "          'attention_mask': encoding['attention_mask'].flatten(),\n",
        "          'label': torch.tensor(label, dtype=torch.long)\n",
        "      }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_5BszaU_s-9"
      },
      "source": [
        "#DataLoader\n",
        "def data_loader(df, tokenizer, max_len, batch_size):\n",
        "  dataset = IMDBDataset(\n",
        "      review = df.label2.to_numpy(),\n",
        "      label = df.label1.to_numpy(),\n",
        "      tokenizer = tokenizer,\n",
        "      max_len = MAX_LEN\n",
        "  )\n",
        "\n",
        "  return DataLoader(dataset, batch_size = BATCH-SIZE, num_workers = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "921mfaVOEyFM"
      },
      "source": [
        "#Split Data\n",
        "df_train, df_test = train_test_split(df, test_size = 0.2, random_state = RANDOM_SEED)\n",
        "\n",
        "train_data_loader = data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf825fGsHZJY"
      },
      "source": [
        "#EL MODELO\n",
        "\n",
        "class BERTArticleClassificator(nn.Module):\n",
        "\n",
        "  def __init__(self, numClases):\n",
        "    super(BERTArticleClassification, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRETRAINED_BERT_MODEL)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.linear = nn.Linear(self.bert.config.hidden_size, numClases)\n",
        "\n",
        "  def forward(self, input_ids, attention_masks):\n",
        "    _, cls_output = self.bert(\n",
        "        input_ids = input_ids,\n",
        "        attention_mask = attention_mask\n",
        "    )\n",
        "\n",
        "    drop_out = self.drop(cls_output)\n",
        "    output = self.linear(drop_out)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kEw2N0gOruk"
      },
      "source": [
        "model = BERTArticleClassification(NCLASSES)\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBx_hC2A_UOp"
      },
      "source": [
        "#PRUEBAS\n",
        "\n",
        "np.random"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}